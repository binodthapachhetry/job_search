job_search:
  description: |
    **Objective:** Find active job postings matching the query '{query}', scrape their details, and format them as JSON.
    **Tools:** You have a 'SerperDevTool' tool and a 'SpiderTool' for scraping.
    **Process:**
    1.  **Search:** Use the 'SerperDevTool' tool with the exact query '{query}' to get a list of potential URLs.
    2.  **Initial Filter:** Review the search results (URLs, titles, snippets). Discard URLs that are clearly *not* job postings (e.g., news articles, general company pages, forum links). Prioritize URLs from known job boards (LinkedIn, Indeed, Glassdoor, company career pages).
    3.  **Scrape & Analyze (Per URL):** For each potentially valid job posting URL from step 2:
        a.  Use the 'SpiderTool' with the URL. Provide *only* the URL to the tool.
        b.  **Error Handling:** If the 'SpiderTool' returns an error for a URL, log the error briefly and *immediately skip* to the next URL. Do *not* include failed URLs in the output.
        c.  **Content Analysis:** If scraping succeeds, carefully analyze the returned content (likely Markdown). Determine if the job posting is still active (look for expiry dates, "closed", "filled" indicators). If it appears inactive/expired, discard this URL.
        d.  **Data Extraction:** If the job seems active, extract the following fields *strictly from the scraped content*:
            - `title`: The job title.
            - `company`: The hiring company's name.
            - `location`: The job location.
            - `description`: A concise summary (1-3 sentences) of the job's purpose or main function.
            - `responsibilities`: Key responsibilities listed (summarize if very long).
            - `requirements`: Key qualifications/skills required (summarize if very long).
            - `salary`: Salary information *if explicitly stated*. If not present, use `null` or omit the field. Do *not* estimate.
            - `url`: The original URL that was successfully scraped.
        e.  **Missing Data:** If a field (other than salary) cannot be found in the scraped text, use `null` or omit the field. Do *not* invent data.
    4.  **Compile Results:** Collect all successfully scraped and extracted job data objects into a JSON list.
  expected_output: A valid JSON list where each object represents a successfully scraped, active job posting. Each object MUST contain the fields `title, `company`, `location`, `description`, `responsibilities`, `requirements`, `url`, and optionally `salary`. Field values must be derived *only* from the scraped page content. Ensure the output is a single, valid JSON list structure.

filter_jobs:
  description: |
    **Objective:** Filter the incoming JSON list of job postings to retain only the top 10-15 most relevant ones based on the original user query.
    **Input:** A JSON list of job objects provided in the context (`{context}`). Each object contains fields like `title`, `description`, `responsibilities`, `requirements`, etc.
    **Reference Query:** The original user query was: '{query}'.
    **Constraint:** You MUST NOT use any external tools. Perform the filtering based *solely* on the text data within the provided job objects in the context.
    **Process:**
    1.  Analyze the `title`, `description`, `responsibilities`, and `requirements` fields of EACH job object in the input list.
    2.  Compare the content of these fields against the keywords, concepts, and intent expressed in the **Reference Query**.
    3.  Assign a relevance score (internal thought process, not needed in output) to each job based on the closeness of the match.
    4.  Select the top 10 to 15 job objects with the highest relevance scores. If fewer than 10 jobs are in the input, return all of them.
  expected_output: A valid JSON list containing *only* the selected top 10-15 most relevant job objects from the input context. The structure and all fields of the selected job objects MUST be preserved exactly as they were in the input list. The output must be a single, valid JSON list.

job_rating:
  description: |
    **Objective:** Rate each job in the provided list based on its suitability against a given resume.
    **Input:** A JSON list of job objects provided in the context (`{context}`).
    **Tool:** You have a 'FileReadTool' to read the resume content.
    **Process:**
    1.  **Read Resume:** Use the 'FileReadTool' to read the content of the resume file specified in the tool's configuration. Store this content for comparison.
    2.  **Iterate & Rate:** For EACH job object in the input JSON list:
        a.  **Compare:** Analyze the job's `title`, `description`, `responsibilities`, and `requirements` fields. Compare the skills, experience, and keywords mentioned in these fields against the content of the resume read in step 1.
        b.  **Determine Rating:** Assign a numerical suitability `rating` from 1 (poor fit) to 10 (excellent fit). The rating should reflect how well the candidate's profile (from the resume) matches the job's stated needs.
        c.  **Justify Rating:** Write a concise `rating_description` (1-2 sentences) explaining the key reasons for the assigned rating (e.g., "Strong match for Python and cloud skills, but lacks required 5+ years management experience.").
        d.  **Augment Job Object:** Add the calculated `rating` (integer) and `rating_description` (string) fields to the *original* job object.
    3.  **Compile Results:** Collect all the augmented job objects into a new JSON list.
    **Constraint:** Ensure that ALL original fields and values from the input job objects are preserved in the output. Only the `rating` and `rating_description` fields should be added. Do not modify other fields.
  expected_output: A valid JSON list containing ALL job objects from the input context. Each job object MUST be augmented with two new fields `rating` (integer 1-10) and `rating_description` (string). All other original fields and values must be unchanged.

evaluate_company:
  description: |
    **Objective:** Evaluate the hiring company for each job in the list and add a company rating.
    **Input:** A JSON list of job objects provided in the context (`{context}`), each containing a `company` field.
    **Tool:** You have a 'DuckDuckGoSearchRun' tool.
    **Process:**
    1.  **Iterate Through Jobs:** For EACH job object in the input JSON list:
        a.  **Identify Company:** Get the company name from the job's `company` field.
        b.  **Research Company:** Use the 'DuckDuckGoSearchRun' tool to search for information about this specific company. Focus queries on aspects relevant to job suitability, such as: recent news about the company, employee reviews (e.g., search "Glassdoor reviews [Company Name]"), general company reputation, and recent major developments (layoffs, growth, funding).
        c.  **Synthesize Findings:** Briefly summarize the key findings from your research regarding the company's stability, culture (based on reviews), and overall reputation.
        d.  **Determine Rating:** Based on your findings, assign a `company_rating` from 1 (very poor outlook/reviews) to 10 (excellent outlook/reviews). Consider factors like positive employee sentiment, company growth/stability, and absence of major negative news.
        e.  **Justify Rating:** Write a concise `company_rating_description` (1-2 sentences) explaining the basis for the rating (e.g., "High rating due to strong growth reported in Q1 and positive Glassdoor reviews mentioning work-life balance.").
        f.  **Augment Job Object:** Add the calculated `company_rating` (integer) and `company_rating_description` (string) to the *original* job object.
    2.  **Compile Results:** Collect all the augmented job objects into a new JSON list.
    **Constraint:** Ensure that ALL original fields and values from the input job objects (including the job `rating` and `rating_description` added in the previous step) are preserved. Only the `company_rating` and `company_rating_description` fields should be added in this step.
  expected_output: A valid JSON list containing ALL job objects from the input context. Each job object MUST be augmented with two new fields `company_rating` (integer 1-10) and `company_rating_description` (string). All other original fields and values must be unchanged. The final structure should align with the job objects within the provided {output_schema}.

structure_results:
  description: |
    **Objective:** Ensure the final list of job objects conforms precisely to the required output JSON schema.
    **Input:** The final JSON list of job objects provided in the context (`{context}`) after all previous processing steps (search, filter, job rating, company rating).
    **Constraint:** You MUST NOT use any tools. Do not add, remove, or substantively change any data fields or values unless absolutely necessary for schema validation. This task is primarily for final formatting and validation.
    **Process:**
    1.  Receive the JSON list of fully processed job objects from the context.
    2.  Verify that the overall structure is a JSON object containing a key (e.g., "jobs") whose value is the list of job objects.
    3.  Verify that each job object in the list contains all the required fields as defined in the target schema {output_schema} and that the data types are correct (e.g., ratings are integers).
    4.  If the structure is already correct, pass it through unchanged. If minor adjustments are needed for schema compliance (e.g., wrapping the list in a root object), perform them.
  expected_output: A single, valid JSON object exactly matching the structure defined by {output_schema}. This typically involves a root key (like "jobs") containing the list of final job objects.
